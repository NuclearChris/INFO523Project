library(httr)
library(jsonlite)
library(readr)
library(dplyr)
library(glue)
library(utils)


###########################
## Step Zero - Get the Data
###########################


# To access NREL Data, we must sign up to get our API Key
# We also need to select a boundary on NREL's Data Viewer to generate location 'points'
# After we have our API parameters, we can send our URL requests for data

API_KEY <- "CjTsm5Nz9Bk6KKB5im8QGvtESmRiio2PGDETn3Vs"
EMAIL <- "nuclearchris@outlook.com"
BASE_URL <- "https://developer.nrel.gov/api/nsrdb/v2/solar/psm3-2-2-download.json?"
# These points are generated by selecting a boundary on NREL's Data Viewer
# https://nsrdb.nrel.gov/data-viewer
# See the README for more details on how to generate your own points for your region
POINTS <- list(
  c('343858', '343859', '344741', '344742', '344743', '344744', '344745', 
    '345625', '345626', '345627', '345628', '345629', '345630', '346511', 
    '346512', '346513', '346514', '346515', '346516', '346517', '347408', 
    '347409', '347410', '347411', '347412', '347413', '347414', '347415', 
    '348308', '348309', '348310', '348311', '348312', '348313', '348314', 
    '348315', '349210', '349211', '349212', '349213', '349214', '349215', 
    '349216', '350112', '350113', '350114', '350115', '350116', '350117', 
    '351007', '351008', '351009', '351010')
)

#This function accepts an argument 'response' and handles errors in case of faulty URL generation
get_response_json_and_handle_errors <- function(response) {
  if (http_status(response)$category != "Success") {
    print(glue("An error has occurred with the server or the request. The request response code/status: {status_code(response)} {http_status(response)$message}"))
    print(glue("The response body: {content(response, 'text')}"))
    stop("Request failed")
  }
  response_json <- fromJSON(content(response, "text", encoding = "UTF-8"), flatten = TRUE)
  
  if (length(response_json$errors) > 0) {
    errors <- paste(response_json$errors, collapse = "\n")
    print(glue("The request errored out, here are the errors: {errors}"))
    stop("Request failed")
  }
  return(response_json)
}

# This builds our request body, where we ask the API for certain attributes with 60 minute resolution
# We also need to attach our API key and email here (defined as variables above)
input_data <- list(
  attributes = 'dni,ghi,dhi,air_temperature,wind_speed',
  interval = '60',
  to_utc = 'false',
  api_key = API_KEY,
  email = EMAIL
)

# Here we define the years of data to request from NREL, each year is one request
start_year <- as.integer(readline(prompt = "Enter the start year of data to load from NREL (e.g., 2020): "))
end_year <- as.integer(readline(prompt = "Enter the end year (e.g., 2022): "))
years <- start_year:end_year

# When we send a request to NREL, their server returns a URL where we can download our .zip folder
# Here I've initialized an empty list to store the download URLs returned by NREL
download_urls <- list()

# For each year, I'm going to send a request with my API parameters (key, email, points, attributes, etc.)
for (name in years) {
  print(glue("Processing name: {name}"))
  #for each 'point' (location) we'll add the year of data we want
  for (id in seq_along(POINTS)) {
    input_data$names <- as.character(name)
    # Collapse location_ids into a single comma-separated string as NREL expects
    input_data$location_ids <- paste(POINTS[[id]], collapse = ",")
    
    print(glue("Making request for point group {id} of {length(POINTS)}..."))
    #send the request in the form of POST, where our body contains all of our parameters
    response <- POST(BASE_URL, body = input_data, encode = "form", add_headers(`x-api-key` = API_KEY))
    # The response includes the download URL, along with a bunch of stuff we don't need
    data <- get_response_json_and_handle_errors(response)
    download_url <- data$outputs$downloadUrl
    #Verify that we retrieved a URL properly
    print(data$outputs$message)
    print(glue("Data can be downloaded from this URL when ready: {download_url}"))
    # Append our list of download URLs that we'll run through later to get our data
    download_urls <- append(download_urls, download_url)  
    # Delay for 10 seconds per year to prevent rate limiting
    Sys.sleep(10)
  }
  print("URLs for all years & points have been requested and recieved!")
}

# After the loop completes, the download_urls list contains all the URLs
# This print statement just shows us the URLs for troubleshooting purposes
print("All download URLs:")
print(download_urls)


# Create a folder in our working directory to save the downloaded files if it doesn't exist
nrel_data_dir <- "NREL_Data"

# Create the directory if it doesn't exist, otherwise empty it
if (!dir.exists(nrel_data_dir)) {
  dir.create(nrel_data_dir)
} else {
  # If the directory already exists, delete all files within it
  files <- list.files(nrel_data_dir, full.names = TRUE)
  file.remove(files)
}

# Assuming that download_urls and years are aligned in the same order
# Iterate over each download URL and download the corresponding file
# The folder year is not actually important, because later we empty & delete these folders
for (i in seq_along(download_urls)) {
  url <- download_urls[[i]]
  
  # Extract the corresponding year based on the loop index
  year <- years[ceiling(i / length(POINTS))]  # Divide i by the number of POINTS to align the year
  
  # Create a unique folder name that includes the year
  file_name <- paste0("NREL_Data/file_", year, "_", i, ".zip")
  
  # Sometimes it takes a few minutes for NREL to prepare the data
  # If we try to access the link, it might return a 404 error
  # To fix this, I keep trying until it returns a 200 response code below
  
  # Initialize the flag to check if the file is successfully downloaded
  file_downloaded <- FALSE
  
  # Loop until the file is successfully downloaded
  while (!file_downloaded) {
    # Check if the URL is accessible
    response <- HEAD(url)
    
    if (status_code(response) == 200) {
      # If the URL is available, download the file
      download.file(url, destfile = file_name, mode = "wb")
      print(glue("Downloaded {file_name}"))
      # Mark the file as successfully downloaded to exit the loop
      file_downloaded <- TRUE
      
    } else if (status_code(response) == 404) {
      # If the URL returns a 404 error, wait one minute and try again
      print(glue("File {file_name} not available yet. Waiting for 1 minute..."))
      Sys.sleep(60)  # Wait for 60 seconds
    } else {
      # If there is another error, print the status and break the loop
      print(glue("Unexpected status code {status_code(response)} for {file_name}."))
      break
    }
  }
}
print("NREL data request & download sequence complete!")


##############################
## Step One - Data Integration
##############################

#create a directory to output postprocessed data
outputData_dir <- "Output_Data"
# Create the directory if it doesn't exist
if (!dir.exists(outputData_dir)) {
  dir.create(outputData_dir)
} else {
  # If the directory already exists, delete all files within it
  files <- list.files(outputData_dir, full.names = TRUE)
  file.remove(files)
}

# Now we have a zip file for each year of data, we need to unpack them
# List all the downloaded zip files in the NREL_Data directory
zip_files <- list.files("NREL_Data", pattern = "\\.zip$", full.names = TRUE)

# Extract each zip file into the NREL_Data directory and then delete the zip file
for (zip_file in zip_files) {
  # Extract the contents of the zip file
  unzip(zip_file, exdir = "NREL_Data")
  print(glue("Extracted {zip_file}"))
  # Delete the zip file after extraction
  file.remove(zip_file)
  print(glue("Deleted {zip_file}"))
}

print("All zip files have been extracted, folders are in the NREL_Data folder.")


# Now we need to empty our folders for each year, and get all the data in one folder
# List all the directories within the NREL_Data folder
folders <- list.dirs("NREL_Data", full.names = TRUE, recursive = FALSE)

# Move all files from each folder to the NREL_Data directory
for (folder in folders) {
  # List all files in the current folder
  files <- list.files(folder, full.names = TRUE)
  
  # Move each file back one directory level to the NREL_Data folder
  for (file in files) {
    file.rename(file, file.path("NREL_Data", basename(file)))
  }
  # Optionally, remove the now-empty folder
  unlink(folder, recursive = TRUE)
  print(glue("Moved files from {folder} to NREL_Data and deleted the folder"))
}

print("All files have been moved to the NREL_Data folder")

###############################
### Part Three - Data Selection
###############################

# Now we have some full years of data downloaded
# Lets get the data we're interested in, and move it to a dataframe
# Prompt the user for a range of years, months, and days
start_year <- as.integer(readline(prompt = "Enter the start year (e.g., 2020): "))
start_month <- as.integer(readline(prompt = "Enter the start month (1-12): "))
start_day <- as.integer(readline(prompt = "Enter the start day (1-31): "))

end_year <- as.integer(readline(prompt = "Enter the end year (e.g., 2022): "))
end_month <- as.integer(readline(prompt = "Enter the end month (1-12): "))
end_day <- as.integer(readline(prompt = "Enter the end day (1-31): "))

# Generate a pattern to match filenames including the years within the range
year_pattern <- paste0("_", start_year:end_year, "\\.csv$", collapse = "|")
# List all CSV files in the NREL_Data directory that match the selected year range
csv_files <- list.files("NREL_Data", pattern = year_pattern, full.names = TRUE)

# Initialize an empty list to store each filtered data frame
filtered_data_list <- list()

# Process each file
for (file in csv_files) {
  # Extract the coordinates from the file name (assuming it's in the format: id_lat_lon_year.csv)
  file_info <- strsplit(basename(file), "_")[[1]]
  latitude <- as.numeric(file_info[2])
  longitude <- as.numeric(file_info[3])
  # Read the CSV file, skipping the first two rows
  data <- read.csv(file, skip = 2, header = FALSE)
  # Get the column names from the first row of data (which is now the first row in the dataframe)
  colnames(data) <- data[1, ]
  # Remove the first row which was used for headers
  data <- data[-1, ]
  # Convert columns back to the correct types
  data <- type.convert(data, as.is = TRUE)
  # Add columns for latitude and longitude, since now it's only in the filename which we're changing
  # This will be important for taking averages by location
  data$Latitude <- latitude
  data$Longitude <- longitude
  # Filter rows based on the year, month, and day range across multiple years
  filtered_data <- data %>%
    filter(
      (Year > start_year | 
         (Year == start_year & (Month > start_month | (Month == start_month & Day >= start_day)))) &
        (Year < end_year | 
           (Year == end_year & (Month < end_month | (Month == end_month & Day <= end_day))))
    )
  # Append the filtered data to the list
  filtered_data_list[[file]] <- filtered_data
}
# Combine all filtered data frames into one
finalData <- bind_rows(filtered_data_list)
# Define the output file name
output_file_name <- paste0("NREL_Data/filtered_data_", start_year, "_", start_month, "_", start_day, "_to_", end_year, "_", end_month, "_", end_day, ".csv")

# Save the filtered data to a new CSV file
# This only works for ~ 1,000,000 rows or less
#write.csv(finalData, output_file_name, row.names = FALSE)
#print(glue("Filtered data has been saved to {output_file_name}."))
#finalData

###################################
### Part Four - Data Transformation
###################################

# Since we're not concerned with single values (60 minute resolution)
# We'll get daily, monthly, and yearly averages for visualization


# Create daily averages per coordinate
dailyAverages <- finalData %>%
  group_by(Latitude, Longitude, Year, Month, Day) %>%
  summarise(
    Avg_DNI = mean(as.numeric(DNI), na.rm = TRUE),
    Avg_GHI = mean(as.numeric(GHI), na.rm = TRUE),
    Avg_DHI = mean(as.numeric(DHI), na.rm = TRUE),
    Avg_Temperature = mean(as.numeric(Temperature), na.rm = TRUE),
    Avg_Wind_Speed = mean(as.numeric(`Wind Speed`), na.rm = TRUE)
  )

# Save daily averages to a CSV file
daily_file_name <- paste0("Output_Data/dailyAverages_", start_year, "_", start_month, "_", start_day, "_to_", end_year, "_", end_month, "_", end_day, ".csv")
write.csv(dailyAverages, daily_file_name, row.names = FALSE)

print(glue("Daily averages have been saved to {daily_file_name}."))

# Create monthly averages per coordinate
monthlyAverages <- finalData %>%
  group_by(Latitude, Longitude, Year, Month) %>%
  summarise(
    Avg_DNI = mean(as.numeric(DNI), na.rm = TRUE),
    Avg_GHI = mean(as.numeric(GHI), na.rm = TRUE),
    Avg_DHI = mean(as.numeric(DHI), na.rm = TRUE),
    Avg_Temperature = mean(as.numeric(Temperature), na.rm = TRUE),
    Avg_Wind_Speed = mean(as.numeric(`Wind Speed`), na.rm = TRUE)
  )

# Save monthly averages to a CSV file
monthly_file_name <- paste0("Output_Data/monthlyAverages_", start_year, "_", start_month, "_", start_day, "_to_", end_year, "_", end_month, "_", end_day, ".csv")
write.csv(monthlyAverages, monthly_file_name, row.names = FALSE)

print(glue("Monthly averages have been saved to {monthly_file_name}."))

# Create yearly averages per coordinate
yearlyAverages <- finalData %>%
  group_by(Latitude, Longitude, Year) %>%
  summarise(
    Avg_DNI = mean(as.numeric(DNI), na.rm = TRUE),
    Avg_GHI = mean(as.numeric(GHI), na.rm = TRUE),
    Avg_DHI = mean(as.numeric(DHI), na.rm = TRUE),
    Avg_Temperature = mean(as.numeric(Temperature), na.rm = TRUE),
    Avg_Wind_Speed = mean(as.numeric(`Wind Speed`), na.rm = TRUE)
  )

# Save yearly averages to a CSV file
yearly_file_name <- paste0("Output_Data/yearlyAverages_", start_year, "_", start_month, "_", start_day, "_to_", end_year, "_", end_month, "_", end_day, ".csv")
write.csv(yearlyAverages, yearly_file_name, row.names = FALSE)

print(glue("Yearly averages have been saved to {yearly_file_name}."))

str(csv_files)

return(list(finalData = finalData, 
            dailyAverages = dailyAverages, 
            monthlyAverages = monthlyAverages, 
            yearlyAverages = yearlyAverages))

###############################
#  Data Organization Complete!
###############################

